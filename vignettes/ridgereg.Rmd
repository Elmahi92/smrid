---
title: "ridgereg"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{ridgereg}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

**Step 1:** Load all the necessary packages.

```{r setup, message=FALSE}
library(smlm)
library(caret)
library(MASS)
library(leaps)
set.seed(1234)
```

**Step 2:** Load and divide the BostonHousing data into a test and a training set.

```{r step2, message=FALSE}
df <- Boston
index <- createDataPartition(df$crim, times = 1, p = 0.8, list = FALSE)
train <- df[index,]
test <- df[-index,]
```

**Step 3:** Use the training data to fit

**a)** a linear regression model

```{r step3a, message=FALSE}
lm_mod <- train(medv ~ ., data = train, method = "lm")
```

**b)** a linear regression model with forward selection of covariates

```{r step3b, message=FALSE}
forward_mod <- train(medv ~ ., data = train, method = "leapForward")
```

**c)** a ridge regression model using the ridgereg() function and find the best hyperparameter value for lambda using 10-fold cross-validation.

```{r step3c, message=FALSE}
rdg_reg <- list(type = "Regression",
                library = "smlm",
                loop = NULL)

rdg_reg$parameters <- data.frame(parameter = c("lambda"),
                      class = c("numeric"),
                      label = c("Lambda"))

rdg_reg$grid <- function(x, y, len = NULL, search = "grid") {
  out <- expand.grid(lambda = seq(from = 0, to = 1, by = 0.1))
  out
}

rdg_reg$fit <- function(x, y, wts, param, lev, last, weights, classProbs, ...) { 
  df <- as.data.frame(cbind(x, y))
  mod_formula <- as.formula(paste0("y~", paste(colnames(df)[1:(ncol(df)-1)], collapse = "+")))
  mod <- ridgereg(data = df, formula = mod_formula, lambda = param$lambda)
}

rdg_reg$predict <- function(modelFit, newdata, preProc = NULL, submodels = NULL) {
  modelFit$predict(modelFit, newdata)
}

rdg_reg$prob <- list(NULL)

ridge_mod <- train(medv ~ ., data = train, 
                   method = rdg_reg,
                   trControl = trainControl(method = "repeatedcv", repeats = 3))
```

The λ value that results in the lowest RMSE with the training dataset is 	λ=1.

**Step 4:** Evaluate the performance of all three models on the test dataset.

```{r step4}
lm_pred <- predict(lm_mod, test)
forward_pred <- predict(forward_mod, test)
ridge_pred <- predict(ridge_mod, test)

RMSE(lm_pred, test$medv)
RMSE(forward_pred, test$medv)
RMSE(ridge_pred, test$medv)
```

The ridge regression model results in the lowest RMSE, and hence fits the test dataset best. The linear regression model where the covariates were selected with forward selection is slightly worse in terms of RMSE, however the model contains fewer covariates and might therefore be easier to understand and interpret.
